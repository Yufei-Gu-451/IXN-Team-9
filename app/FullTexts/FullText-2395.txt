The analysis of array-valued data, or tensor data, is of interest to numerous fields, including psychometrics (Kiers and Mechelen, 2001), chemometrics (Smilde et al., 2005;Bro, 2006), imaging (Vasilescu and Terzopoulos, 2003), signal processing (Cichocki et al., 2014) and machine learning (Tao et al., 2005), among others (Kroonenberg, 2008;Kolda and Bader, 2009). Such data consist of measurements indexed by multiple categorical factors. For example, multivariate measurements on experimental units over time may be represented by a three-way arrayX= {xNone} ? RNone*None*None, withiindexing units,jindexing variables andtindexing time. Another example is multivariate relational data, wherexNoneis the type-krelationship between personiand personj. Statistical analysis of such data often proceeds by fitting a model such asX= Theta +E, where Theta is low-dimensional andErepresents additive residual variation about Theta. Standard models for Theta include regression models, additive effects models (such as those estimated by ANOVA decompositions) and unconstrained mean models if replicate observations are available. Another popular approach is to model Theta as being a low-rank array. For such models, ordinary least-squares estimates of Theta can be obtained via various types of tensor decompositions, depending on the definition of rank being used (De Lathauwer et al., 2000b,a;de Silva and Lim, 2008). Less attention has been given to the analysis of the residual variationE. However, estimating and accounting for such variation is critical for a variety of inferential tasks, such as prediction, model-checking, construction of confidence intervals, and improved parameter estimation over ordinary least squares. One model for variation among the entries of an array is the array normal model (Akdemir and Gupta, 2011;Hoff, 2011) which is an extension of the matrix normal model (Srivastava and Khatri, 1979;Dawid, 1981), often used in the analysis of spatial and temporal data (Mardia and Goodall, 1993;Shitan and Brockwell, 1995;Fuentes, 2006). The array normal model is a class of normal distributions that are generated by a multilinear operator known as the Tucker product: A randomK-way arrayXtaking values in RNoneNone*? *Nonehas an array normal distribution ifNone, where "*" denotes the Tucker product (described further in Section 2),Zis a random array in RNoneNone*? *Nonehaving i.i.d. standard normal entries, andANoneis apNone*pNonenonsingular matrix for eachk? {1, ...,K}. LettingNoneand "?" denote the Kronecker product, we writeNoneNoneA maximum likelihood estimate (MLE) for the parameters in(1)can be obtained via an iterative coordinate descent algorithm (Hoff, 2011), which is a generalization of the iterative "flip-flop" algorithm developed inMardia and Goodall (1993)andDutilleul (1999), or alternatively the optimization procedures described inWiesel (2012b). However, based on results for the multivariate normal model, one might suspect that the MLE lacks desirable optimality properties: In the multivariate normal model,James and Stein (1961)showed that the MLE of the covariance matrix is neither admissible nor minimax. This was accomplished by identifying a minimax and uniformly optimal equivariant estimator that is different from the (equivariant) MLE, and therefore dominates the MLE. As pointed out by James and Stein, this equivariant estimator is itself inadmissible, and improvements to this estimator have been developed and studied byStein (1975);Takemura (1984);Lin and Perlman (1985), andHaff (1991), among others. This article develops similar results for the array normal model. In particular, we obtain a procedure to obtain the uniformly minimum risk equivariant estimator (UMREE) under a lower-triangular product group of transformations for which the model(1)is invariant. Unlike for the multivariate normal model, there is no simple characterization of this class of equivariant estimators. However, results ofZidek (1969)andEaton (1989)can be used to show that the UMREE can be obtained from the Bayes decision rule under an improper prior, which we derive in Section 2. In Section 3 we obtain the posterior distribution under this prior, and show how it can be simulated from using a Markov chain Monte Carlo (MCMC) algorithm. Specifically, the MCMC algorithm is a Gibbs sampler that involves simulation from a class of distributions over covariance matrices, which we call the "mirror-Wishart" distributions. In Section 4.1 we develop a version of Stein's loss function for covariance estimation in the array normal model, and show how the Gibbs sampler of Section 3 can be used to obtain the UMREE for this loss. We discuss an orthogonally equivariant improvement to the UMREE in Section 4.2, which can be seen as analogous to the estimator studied byTakemura (1984). Section 4.3 compares the risks of the MLE, UMREE and the orthogonally equivariant estimator as a function of the dimension ofXin a small simulation study. A discussion follows in Section 5. Proofs are contained in anappendix. The array normal model on RNoneNone*? *Noneconsists of the distributions of randomK-arraysX? RNoneNone*? *Nonefor whichNoneNonefor some Theta ? RNoneNone*? *None, nonsingular matricesANone? RNone*None,k= 1, ...,Kand a randomp1* ? *pNonearrayZwith i.i.d. standard normal entries. Here, "*" denotes theTucker product, which is defined by the identityNoneNonewhere "?" is the Kronecker product andz= vec(Z), the vectorization ofZ. This identity can be used to find the covariance of the elements of a random array satisfying(2): Lettingx,z, theta be the vectorizations ofX,Z, Theta, we haveNoneand so the array normal distributions correspond to the multivariate normal distributions with separable (Kronecker structured) covariance matrices. A useful operation related to the Tucker product is thematricizationoperation, which reshapes an array into a matrix along an index set, ormode. For example, themode-k matricizationofZis thepNone* (? None:None? NonepNone)-dimensional matrixZ(None)having rows equal to the vectorizations of the "slices" ofZalong thekth index set. An important identity involving the Tucker product is that ifY=Z* {A1, ...,ANone} thenNoneNoneAs shown inHoff (2011), a direct application of this identity givesNonewherecNoneis a scalar. This shows thatNonecan be interpreted as the covariance among thepNoneslices of the arrayXalong itskth mode. The array normal model can be parameterized in terms of a mean array E[X] = Theta ? RNoneNone*? *Noneand covariance Cov[vec(X)] = sigma2(SigmaNone? ? ? Sigma1), where sigma2 greater than 0 and for eachk,None, the set ofpNone*pNonepositive definite matrices. To make the parameterization identifiable, we restrict the determinant of each SigmaNoneto be one. Denote byNonethis parameter space, that is, the values of (sigma2, Sigma1, ..., SigmaNone) for which |SigmaNone| = 1,k= 1, ...,K. Under this parameterization, we writeX~NNoneNone*? *None(Theta, sigma2(SigmaNone? ? ? Sigma1)) if and only ifNone, where for eachk, PsiNoneis a matrix such thatNone. Given a sampleX1, ...,XNone~ i.i.d. NNoneNone*? *None(Theta, sigma2(SigmaNone? ? ? Sigma1)), the (K+1)-arrayXobtained by "stacking"X1, ...,XNonealong a (K+1)st mode also has an array normal distribution,Nonewhere1Noneis then* 1 vector of ones and "?" denotes the outer product. Ifn greater than 1 then covariance estimation for the array normal model can be reduced to the case that Theta = 0. To see this, letHbe a (n- 1) *nmatrix such thatHHNone=INone-1andH1None= 0. This implies thatNone. LettingY=X* {INoneNone, ...,INone,H}, andY(None+1)be the mode-(K+ 1) matricization ofY, we have E[Y(None+1)] =HE[X(None+1)] =H1Nonevec(Theta)None=0, and soYis mean-zero. Using identity(3), the covariance of vec(Y) can be shown to be sigma2(HHNone? SigmaNone? ? ? Sigma1) = sigma2(INone-1? SigmaNone? ? ? Sigma1), and soY~NNoneNone*? *None* (None-1)(0, sigma2(INone-1? SigmaNone? ? ? Sigma1)). For the remainder of this paper, we consider covariance estimation in the case that Theta = 0. Consider the model for an i.i.d. sample of sizenfrom ap-variate mean-zero multivariate normal distribution,X~NNone*None(0,INone? Sigma),None. Recall thatAX~NNone*None(0,INone? ASigmaANone) for nonsingular matricesA, and so in particular this model is invariant under left multiplication ofXby elements ofNone, the group of lower triangular matrices with positive diagonals. An estimator Sigma? mapping the sample space RNone*NonetoNoneis said to be equivariant under this group if Sigma? (AX) =ASigma? (X)ANonefor allNoneandX? RNone*None. James and Stein (1961)characterized the class of equivariant estimators for this model, identified the UMREE under a particular loss function and showed that the UMREE is minimax. Additionally, as the MLEXXNone/nis equivariant and different from the UMREE, the MLE is dominated by the UMREE. We pursue analogous results for the array normal model by first reparameterizing in terms of the parameter Sigma1/2= (sigma, Psi1, ..., PsiNone), soNoneNonewhere sigma greater than 0 and each PsiNoneis in the setNoneofpNone*pNonelower triangular matrices with positive diagonals and determinant 1. In this parameterization, PsiNoneis the lower triangular Cholesky square root of the mode-kcovariance matrix SigmaNonedescribed in Section 2.1. Define the groupNoneasNonewhere the group operation isNoneNote thatNoneconsists of the same set as the parameter space for the model, as parameterized in(5). If the groupNoneacts on the sample space byNonethen as shown inHoff (2011)it acts on the parameter space byNonewhich we write concisely asg: Sigma1/2? ASigma1/2. An estimator, Sigma? 1/2= (sigma?, Psi? 1, ..., Psi? None), mapping the sample space RNoneNone*? *None*Noneto the parameter spaceNoneis equivariant ifNoneFor example, if Psi? Noneis the estimator of PsiNonewhen observingX, thenANonePsi? Noneis the estimator when observingaX* {A1...,ANone,INone}. Unlike the case for the multivariate normal model, the class ofNone- equivariant estimators for the array normal model is not easy to characterize beyond the definition given above. However, in cases like the present one where the group space and parameter space are the same, the UMREE under an invariant loss can be obtained as the generalized Bayes decision rule under a (generally improper) prior obtained from a right invariant (Haar) measure over the group (Zidek, 1969;Eaton, 1989). The first step towards obtaining the UMREE is then to obtain a right invariant measure and corresponding prior. To do this, we first need to define an appropriate measure space for the elements ofNone. Recall that matricesANoneinNonehave determinant 1, and so one of the nonzero elements ofANonecan be expressed as a function of the others. For the rest of this section and the next, we parameterizeNonein terms of the elements {ANone[None]: 2 less than =i less than =pNone, 1 less than =j less than =i}, and express the upper-left elementANone[1,1]as a function of the other diagonal elements, so thatNone. The "free" elements ofNonetherefore take values in the space ? None= {aNone greater than 0,aNone? R: 2 less than =i less than =pNone, 1 less than =j less than i}. A right invariant measure over the groupNoneisNonewhere dmuis Lebesgue measure overR+* ? NoneNone* ? * ? None. We note that although the density given above is specific to the particular parameterization of theNone, the inference results that follow will hold for any parameterization. LetNonebe an invariant loss function, so thatL(Sigma1/2,B) =L(ASigma1/2,AB) for allA,BandNone. Theorem 6.5 ofEaton (1989)implies that the value of the UMREE when the arrayXis observed is the minimizer inB= (b,B1, ...,BNone) of the integralNonewhereNoneis the array normal density at the parameter valueNoneandNoneis an arbitrary element ofNone. Since the group action is transitive over the parameter space, and since the integral is right invariant,Nonecan be chosen to be equal to (1,INoneNone, ...,INone). Furthermore, since the parameter space and group space are the same, replacingAwith Sigma1/2in the above integral indicates that the UMREE atXis the minimizer inBofNonethat is, the UMREE is the Bayes estimator under the (improper) prior nuNonefor Sigma1/2. This is summarized in the following corollary:For an invariant loss functionNonethe estimatorSigma? 1/2,defined asNoneNoneuniformly minimizes the risk E[L(Sigma1/2, Sigma? 1/2(X))|Sigma1/2]among equivariant estimatorsSigma? 1/2ofSigma1/2. The expectation inNoneis with respect to the posterior densityNoneNonewhereNone. In addition to uniformly minimizing the risk, the UMREE has two additional features. First, since any unique MLE is equivariant (Eaton, 1989, Theorem 3.2), the UMREE dominates any unique MLE, presuming the UMREE is not the MLE. Second, the UMREE underNoneis minimax. This follows becauseNoneis a subgroup ofNone, asNonefor alla greater than 0 andNone. SinceNoneis a solvable group (James and Stein, 1961), this necessarily implies thatNoneis solvable (Rotman, 1995, Theorem 5.15). By the results ofKiefer (1957)andBondar and Milnes (1981), the equivariant estimator that minimizes(6)is minimax. Note that because the prior nuNoneis improper, the posterior(7)is not guaranteed to be proper. However, we are able to guarantee propriety if the sample sizenis sufficiently large:LetNone. For p(sigma, Psi1, ..., PsiNone|X)defined inNone,NoneThe sample size in the Theorem is sufficient for propriety, but empirical evidence suggests that it is not necessary. For example, results from a simulation study in Section 4 suggest that, for some dimensions, a sample size ofn= 1 is sufficient for posterior propriety and existence of an UMREE. For the results in Section 2 to be of use, we must be able to actually minimize the posterior risk inEquation 6under an invariant loss function of interest. In the next section, we will show that the posterior risk minimizer under a multiway generalization of Stein's loss is given by posterior expectations of the form E[(sigma2SigmaNone)-1|X], whereNone. Although these posterior expectations are not generally available in analytic form, they can be approximated using a MCMC algorithm. In this section, we show how a relatively simple Gibbs sampler can be used to simulate a Markov chain of values of Sigma1/2= (sigma, Psi1, ..., PsiNone), having a stationary distribution equal to the desired posterior distribution given byEquation 7. These simulated values can be used to approximate the posterior distribution of Sigma1/2givenX, as well as any posterior expectation, in particular E[(sigma2SigmaNone)-1|X]. The Gibbs sampler proceeds by iteratively simulating values of {sigma, PsiNone} from their full conditional distribution given the current values of {Psi1, ..., PsiNone-1, PsiNone+1, ..., PsiNone}. This is done by simulating sigma2SigmaNonefrom its full conditional distribution, from which sigma and PsiNonecan be recovered. One iteration of the Gibbs sampler proceeds as follows:Iteratively for eachk? {1, ...,K},NoneNonesimulate (sigma2SigmaNone)-1~ mirror-WishartNone(np/pNone,None);set PsiNoneto be the lower triangular Cholesky square root of SigmaNone. In this algorithm,X(None)? RNone*None/Noneis the mode-kmatricization ofXand Psi-None= PsiNone? ? ? PsiNone+1?PsiNone-1? ? ?Psi1. The mirror-Wishart distribution is a probability distribution on positive definite matrices, related to the Wishart distribution as follows:A random q*q positive definite matrix S has a mirror-Wishart distribution with degrees of freedomnu greater than 0and scale matrixNoneifNonewhereVVNoneis the lower triangular Cholesky decomposition of a WishartNone(nu,INone)-distributed random matrix and UUNoneis the upper triangular Cholesky decomposition ofPhi. Some understanding of the mirror-Wishart distribution can be obtained from its expectation:If S~mirror-WishartNone(nu, Phi)thenNonewhere UUNoneis the upper triangular Cholesky decomposition ofPhiand D is a diagonal matrix with entries dNone= (nu +q+ 1 - 2j)/nu,j= 1, ...,q. The calculation follows from Bartlett's decomposition, and is in theappendix. The implications of this for covariance estimation are best understood in the context of the multivariate normal modelX~NNone*None(0,INone? Sigma). In this case, for a given prior the Bayes estimator under Stein's loss is given by E[Sigma-1|X]-1(see, for exampleYang and Berger (1994)). Under Jeffreys' noninformative prior, Sigma-1~ WishartNone(n, (XXNone)-1) and so the Bayes estimator isXXNone/n. While unbiased, this estimator is generally thought of as not providing appropriate shrinkage of the sample eigenvalues. Note that under Jeffreys' prior,a posterioriwe haveNone, whereVVNone~ WishartNone(n,INone) andUUNoneis the upper triangular Cholesky decomposition of (XXNone)-1. In contrast, under a right invariant measure as our prior we haveNone. The expectation ofVVNoneisnI, whereas the expectation ofVNoneVisnD, which provides a different pattern of shrinkage of the eigenvalues ofXXNone. By Lemma 1, the Bayes estimator under a right invariant measure as our prior in this case is given by (nUDUNone)-1=U-NoneD-1U-1/n, which is the UMREE obtained byJames and Stein (1961). Thus, the UMREE in the multivariate normal model corresponds to a Bayes estimator under a right invariant measure as our prior and mirror-Wishart posterior distribution. The Gibbs sampler is based on the full conditional distribution of (sigma2SigmaNone)-1, which we derive from the full conditional density of {sigma, PsiNone}:Nonewhere dependence of the density on {Psi1, ..., PsiNone-1, PsiNone+1, ..., PsiNone,X} has been made implicit. Now setLNone= sigmaPsiNone. The full conditional density ofLNonecan be obtained from that of {sigma, PsiNone} and the Jacobian of the transformation. The Jacobian of the transformation g(sigma, PsiNone) = sigmaPsiNone,mappingNonetoNoneisNoneSinceLNone= sigmaPsiNone, we have sigma = |LNone|1/Noneand PsiNone[None]=LNone[None]/sigma =LNone[None]/|LNone|1/None. Lemma 2 impliesNonewhich, through straightforward calculations, can be shown to be proportional toNoneWe now "absorb"NoneintoLNone. First, take the lower triangular Cholesky decomposition ofNoneso thatNoneWe haveNoneNow letNone, so thatLNone= PhiNoneWNone. This change of variables has JacobianNone(Eaton, 1983, Proposition 5.13), so thatNoneNoneNote that the distribution ofWNonedoes not depend on Psi-None. Now compareequation (8)to the density of the lower triangular Cholesky square rootWof an inverse-Wishart distributed random matrixNonegiven byNoneNoneThe conditional densities of the off-diagonal elements ofWNoneandWgiven the diagonal elements clearly have the same form. The diagonal elements ofWNoneandWin(8)and(9)are square roots of inverse-gamma distributed random variables, but with different shape parameters. To show this, we first derive the conditional densities of the off-diagonal elements ofW:(Bartlett's decomposition for the inverse-Wishart) Let W be the lower triangular Cholesky square root of an inverse-Wishart distributed matrix, so WWNone~inverse-WishartNone(nu,INone). Then for each i= 1, ...,pNone,NoneHere,W[1:(None-1),1:(None-1)]denotes the submatrix ofWmade up of the first (i- 1) rows and columns, andW[None,1:(None-1)]is the vector made up of the first (i- 1) elements of theith row. By Lemma 3, ifWWNone~ inverse-Wishart(np/pNone,INone) then the squared diagonal elements ofWare independent inverse-gamma((np/pNone-pNone+i)/2, 1/2) random variables. This tells us thatNoneThis result allows us to integrate(8)with respect to the off-diagonal elements ofWNone, givingNoneA change of variables implies that theNoneare independent, andNoneNoneThis completes the characterization of the distribution ofWNone: The distribution of the diagonal elements is given by(10)and the conditional distribution of the off-diagonal elements given the diagonal can be obtained from Lemma 3. Finally, this distribution can be related to a Wishart distribution via the following lemma:Let WNonebe a random pNone*pNonelower triangular matrix such thatNoneThen the elements ofNoneare distributed independently asNoneNote that the matrixVNoneis distributed as the lower triangular Cholesky square root of a Wishart distributed random matrix. Applying the lemma toWNone, for which nu =np/pNone, we have thatNoneis equal in distribution to the lower triangular Cholesky square root of a random matrix which is WishartpNone(np/pNone,INone). That is, the precision matrixNoneis conditionally distributed asNoneWe say the matrix,Nonehas amirror-Wishartdistribution becauseNonewould have a Wishart distribution. This completes the derivation of the full conditional distribution ofNone. Although not necessary for posterior approximation, the full conditional distribution of sigma given Psi1, ..., PsiNoneandXis easy to derive. The posterior density isNoneLetting gamma = 1/sigma2, we haveNoneand so the full conditional distribution of 1/sigma2isNoneA commonly used loss function for estimation of a covariance matrix Sigma is Stein's loss,NoneFirst introduced byJames and Stein (1961), Stein's loss has been proposed as a reasonable and perhaps better alternative to quadratic loss for evaluating performance of covariance estimators. For example, Stein's loss, unlike quadratic loss, does not penalize overestimation of the variances more severely than underestimation. Recall from Section 2 that the array normal model can be parameterized in terms ofNone, where |SigmaNone| = 1 for eachk= 1, ...,K. For estimation of the covariance parametersNone, we consider the following generalization of Stein's loss, which we call "multiway Stein's loss":NoneNoneIt is easy to see that forK= 1, multiway Stein's loss reduces to Stein's loss. Multiway Stein's loss also has the attractive property of being invariant under multilinear transformations. To see this, defineSLNoneto be the set of lists of the formA= (a,A1, ...,ANone) for whicha greater than 0 andANone? SLNonefor eachk, withSLNonebeing the special linear group ofpNone*pNonematrices with unit determinant. For two elementsAandBofSLNone, defineAB= (ab,A1B1, ...,ANoneBNone) andNone. Multiway Stein's loss is invariant under transformations of the form Sigma - greater than ASigmaANone, asNoneNotably,(11)is invariant underNone, asNone, so the bestNone-equivariant estimator under multiway Stein's loss can be found using Corollary 1. (UMREE under multiway Stein's loss) LetNonewhere the expectation is with respect to the posterior distribution given byNone. The minimizer of the posterior expectationNonewith respect to s and the SNone's isNoneThe posterior expectationE[(sigma2SigmaNone)-1|X] may be approximated by the Gibbs sampler of Section 3. That is, if (sigma2SigmaNone)(1), ..., (sigma2SigmaNone)(None)is a long sequence of values of (sigma2SigmaNone) simulated from the Gibbs sampler, thenNoneThe form of multiway Stein's loss(11)includes a weighted sum of tr(None),k= 1, ...,K. We note that equivariant estimation of Sigma is largely unaffected by changes to the weights in this sum:Define weighted multiway Stein's loss asNonefor known wNone greater than 0,k= 1, ...,K. Then the UMREE under LNoneis given byNoneThe proof is very similar to that of Proposition 1 and is omitted. This proposition states that only estimation of the scale is affected when we "weight" the loss more heavily for some components of Sigma than others. The posterior distribution may also be used to obtain the UMREE under Stein's original lossLNone, as it too is invariant under transformations of the lower triangular product group. However, risk minimization with respect toLNonerequires additional numerical approximations: Let ? be the unique symmetric square root ofNone, which may be approximated by the Gibbs sampler described in Section 3. Minimization of the risk with respect toLNoneis equivalent to the minimization in (s2,S1, ...,SNone) ofNonewhere ?? ? RNoneNone*? *None*Noneis the array such that ?? (None+1)= ?, andNoneis any square root matrix ofSNone. Iteratively settingNonewill decrease the posterior expected loss at each step. This procedure is analogous to using the iterative flip-flop algorithm to find the MLE based on a sample covariance matrix ofNone. Application of the results from (Wiesel, 2012a) show that the posterior risk has a property known as geodesic convexity, implying that any local minimizer obtained from this algorithm will also be a global minimizer. The estimator in Proposition 1 depends on the ordering of the indices, and so it is not permutation equivariant. Mirroring the ideas studied inTakemura (1984), in this section we derive a minimax orthogonally equivariant estimator (which is necessarily permutation equivariant) that dominates the UMREE of Proposition 1. First, notice that by transforming the data and then back-transforming the estimator, we can obtain an estimator whose risk is equal to that of the UMREE: For Gamma = (1, Gamma1, ..., GammaNone) ? {1} * ? NoneNone* ? * ? None, where ? Noneis the group ofpNonebypNoneorthogonal matrices, letX? =X* {Gamma1, ..., GammaNone}. Then Sigma? (X?) is an estimator of GammaSigmaGammaNoneand Sigma? (X) = GammaNoneSigma? (X? )Gamma is an estimator of Sigma. The risk of this estimator is the same as that of the UMREE Sigma? (X):Nonewhere the second equality follows from the invariance of the loss, the third equality follows from a change of variables, and the last equality follows because the risk of Sigma? is constant over the parameter space. The UMREE Sigma? and the estimator Sigma? have the same risks but are different. Since multiway Stein's loss is convex in each argument, averaging these estimators somehow should produce a new estimator that dominates them both. In the multivariate normal case in whichK= 1, averaging the value of GammaNoneSigma? (GammaX)Gamma with respect to the uniform (invariant) measure for Gamma over the orthogonal group results in the estimator ofTakemura (1984). This estimator is orthogonally equivariant, dominates the UMREE and is therefore also minimax. Constructing an analogous estimator in the multiway case is more complicated, as it is not immediately clear how the back-transformed estimators should be averaged. Direct numerical averaging of estimates of sigma2(Sigma1? ? ? SigmaNone) will generally produce an estimate that is not separable and therefore outside of the parameter space. Similarly, averaging estimates of each SigmaNoneseparately will not work, as the space of covariance matrices with determinant one is not convex. Our solution to this problem is to average a transformed version of Sigma = (sigma2, Sigma1, ..., SigmaNone) for which each SigmaNonelies in the convex set of trace-1 covariance matrices, then transform back to our original parameter space. The resulting estimator, which we call the multiway Takemura estimator (MWTE), is orthogonally equivariant and uniformly dominates the UMREE. Letsigma? 2(Gamma,X)andSigma? None(Gamma,X)be the UMREEs ofsigma2andNonebased on data X* {Gamma1, ..., GammaNone,INone}. LetNoneandNoneLetSigma? None(X) =SNone(X)/|SNone(X)|1/Nonefork= 1, ...,K. Then(sigma? 2(X), Sigma? 1(X), ..., Sigma? None(X))is orthogonally equivariant and uniformly dominates the UMREE of Proposition 1. Note that "averaging" over any subset of ? None1* ? * ? Nonein the manner of Proposition 3 will uniformly decrease the risk. By averaging with respect to the uniform measure over the orthogonal group, we obtain an estimator that has the attractive property of being orthogonally equivariant. In practice it is computationally infeasible to integrate over the space of orthogonal matrices. However, we may obtain a stochastic approximation to the MWTE as follows: Independently for eacht= 1, ...,Tandk= 1, ...,K, simulateNonefrom the uniform distribution on ? None. LetNoneSet Sigma? None(X) =SNone(X)/|SNone(X)|1/Nonefork= 1, ...,K. Then an approximation to the MWTE isNoneNoneThis is a randomized estimator which is orthogonally invariant in the sense of Definition 6.3 ofEaton (1989). We numerically compared the risks of the MLE, UMREE, and the MWTE under several three-way array normal distributions, using a variety of values of (p1,p2,p3) and withn= 1. For each (p1,p2,p3) under consideration, we simulated 100 data arrays from the array normal model. As the risk of both the MLE and the UMREE are constant over the parameter space, it is sufficient to compare their risks at a single point in the parameter space, which we took to be Sigma = (1,INoneNone,INoneNone,INoneNone). Risks were approximated by averaging the losses of each estimator across the 100 simulated data arrays. For each data array, the MLE was obtained from the iterative coordinate descent algorithm outlined in (Hoff, 2011). Each UMREE was approximated based on 1250 iterations of the Gibbs sampler described in Section 3, from which the first 250 iterations were discarded to allow for convergence to the stationary distribution (convergence appeared to be essentially immediate). The ratio of risk estimates across several values of (p1,p2,p3) are are plotted in solid lines inFigure 1. We considered array dimensions in which the first two dimensions were identical. This scenario could correspond to, for example, data arrays representing longitudinal relational or network measurements betweenp1=p2nodes atp3time points. The first panel of the figure considers the relative performance of the estimators as the "number of time points" (p3) increases. The results indicate that the UMREE provides substantial and increasing risk improvements compared to the MLE asp3increases. However, the right panel indicates that the gains are not as dramatic and not increasing when the "number of nodes" (p1=p2) increases whilep3remains fixed. Even so, the variability in the ratio of losses (shown with vertical bars) decreases as the number of nodes increases, indicating an increasing probability that the UMREE will beat the MLE in terms of loss. We also compared these risks to the risk of the approximate MWTE given in(12), withT? {2, 3}. The risks for the approximate MWTE relative to those of the MLE are shown in dashed lines in the two panels of the Figure, and indicate non-trivial improvements in risk as compared to the UMREE. We examined values ofTgreater than 3 but found no appreciable further reduction in the risk. Note, however, that the MWTE does not have constant risk over the parameter space (though MWTE will have constant risk over the orbits of the orthogonal product group). This article has extended the results ofJames and Stein (1961)andTakemura (1984)by developing equivariant and minimax estimators of the covariance parameters in the array normal model. Considering the class of estimators equivariant with respect to a special lower triangular group, we showed that the uniform minimum risk equivariant estimator (UMREE) can be viewed as a generalized Bayes estimator that can be obtained from a simple Gibbs sampler. We obtained an orthogonally equivariant estimator based on this UMREE by combining values of the UMREE under orthogonal transformations of the data. Both the UMREE and the orthogonally equivariant estimator are minimax, and both dominate any unique MLE in terms of risk. Empirical results in Section 4 indicate that the risk improvements of the UMREE over the MLE can be substantial, while the improvements of the orthogonally equivariant estimator over the UMREE are more modest. However, the risk improvements depend on the array dimensions in a way that is not currently understood. Furthermore, we do not yet know the minimal conditions necessary for the propriety of the posterior or the existence of the UMREE. Empirical results from the simulations in Section 4 suggest that the UMREE exists for sample sizes as low asn= 1, at least for the array dimensions in the study. This is similar to the current state of knowledge for the existence of the MLE: The array normal likelihood is trivially bounded forn greater than =p(as it is bounded by the maximized likelihood under the unconstrainedp-variate normal model), and some sufficient conditions for uniqueness of the MLE are given inOhlson et al. (2013). However, empirical results (not shown) suggest that a unique MLE may exist forn= 1 for some array dimensions (although not for others). Obtaining necessary and sufficient conditions for the existence of the UMREE and the MLE is an ongoing area of research of the authors.