many decisions are based onthe balance of evidence that arrives at different points in time. This process is quantified via simple perceptual discrimination tasks in which the momentary value of a sensory signal carries negligible evidence, but correct responses arise from summation of this signal over the duration of a trial. At the core of such decision making must lie neural mechanisms that integrate signals over time (Gold and Shadlen 2007;Wang 2008;Bogacz et al. 2006). The function of these mechanisms is intriguing, because perceptual decisions develop over hundreds of milliseconds to seconds, while individual neuronal and synaptic activity often decays on timescales of several to tens of milliseconds, a difference of at least an order of magnitude. A mechanism that bridges this gap is feedback connectivity tuned to balance, and hence cancel, inherent voltage leak and synaptic decay (Cannon and Robinson 1983;Usher and McClelland 2001). The tuning required for a circuit-based or cellular mechanism to achieve this balance presents a challenge (Seung 1996;Seung et al. 2000), illustrated inFig. 1None,top, via motion of a ball on a smooth energy surface. Here, the ball positionE(t) represents the total activity of a circuit (relative to a baseline marked 0); momentary sensory input perturbsE(t) to increase or decrease. If decay dominates (Fig. 1None,top right), thenE(t) always has a tendency to "roll back" to baseline values, thus forgetting accumulated sensory input. Conversely, if feedback connections are in excess, then activity will grow away from the baseline value (Fig. 1None,top center). If balance is perfectly achieved via fine tuning (Fig. 1None,top left), then temporal integration can occur. That is, inputs can then smoothly perturb network activity back and forth, so that the network state at any given time represents the time integral of past inputs. Schematic of neural integrator models. A: visualizing integration via an energy surface (Pouget and Latham 2002;Goldman et al. 2009). The robust integrator can "fixate" at a range of discrete values, indicated by a sequence of potential wells, despite mistuning of circuit feedback. These wells can be arbitrarily "close" in the energy landscape, providing a mechanism for graded persistent activity. Without these wells (the nonrobust case), activity in a mistuned integrator would either exponentially grow or decay, as attop. Perturbing the robust integrator from one well to the next, however, requires sufficiently strong momentary input. B: as a consequence, low-amplitude segments in the input signal DeltaI(t), below a robustness limitR, are not accumulated by a robust integrator: only the high-amplitude segments are. The piecewise definition ofNonecaptures this robustness behavior, resulting in the accumulated activity shown, and may be related to, e.g., a detailed bistable-subpopulation model. A decision is expressed when the accumulated valueE(t) crosses the decision threshold theta. Koulakov et al. (2002)proposed an alternative, circuit-based model, equivalent to movement along a scalloped energy surface made up of neighboring "energy wells" (Fig. 1None,bottom) (Pouget and Latham 2002;Goldman et al. 2009;Fransen et al. 2006). Importantly, even without finely tuned connectivity, network states can hold prior values without decay or growth, allowing integration of inputs over time. Thus this mechanism is called a robust integrator. However, the energy wells imply a minimum input strength to transition between adjacent states, with inputs below this limit effectively ignored. Intriguingly, this loss of sensitivity is shared by a mechanism of integration that was discovered at the level of single cells. Egorov et al. (2002)observed integration and graded steady states in layer V cells of entorhinal cortex, and movement among these states was only driven by stimuli stronger than a certain limit [as modeled inFransen et al. (2006);Loewenstein and Sompolinsky (2003)]. These studies point to a general issue affecting integrator mechanisms: a loss of sensitivity to weak inputs. As made explicit inKoulakov et al. (2002)andGoldman et al. (2003), this may arise in a tradeoff in which robustness to parameter mistuning is gained. Here, we abstract the underlying mechanisms and present a general theoretical analysis of the tradeoff between sensitivity and robustness for decisions based on integrated evidence. We find that the tradeoff is favorable: decision speed and accuracy are lost when the integrator circuit is mistuned, but this loss is partially recovered by making the network dynamics robust. Thus, although the robust integrator discards the weakest portions of the evidence stream, enough evidence is retained to produce decisions that are faster and more accurate than would occur with unchecked over- or under-tuning of feedback (Fig. 1None). The implication is that cellular or circuit-based robust integrators may be remarkably well suited to subserve a variety of decision-making computations. To explore the consequences of the robust integrator mechanism for decision performance, we begin by constructing a two-alternative decision-making model similar to that proposed byMazurek et al. (2003). For concreteness, we concentrate on the forced choice motion discrimination task (Roitman and Shadlen 2002;Mazurek et al. 2003;Gold and Shadlen 2007;Churchland et al. 2008;Shadlen and Newsome 1996,2001). Here, subjects are presented with a field of random dots, of which a subset move coherently in one direction; the remainder are relocated randomly in each frame. The task is to correctly choose the direction of coherent motion from two alternatives (i.e., left vs. right). As inMazurek et al. (2003)[see alsoSmith (2010)], we first simulate a population of neurons that represent the sensory input to be integrated over time. This population is a rough model of cells in extrastriate cortex (area MT) that encode momentary information about motion direction (Britten et al. 1992,1993;Salzman et al. 1992). We pool spikes from model MT cells that are selective for each of the two possible directions into separate streams, labeled according to their preferred "left" and "right" motion selectivity (seeFig. 2). Two corresponding integrators then accumulate the difference between these streams, left-less-right or vice versa. Each integrator therefore accumulates the evidence for one alternative over the other. The dynamics of these integrators embody the tradeoff between robustness and sensitivity that is the focus of our study (seeNeural Integrator Model and the Robustness-Sensitivity Tradeoff). Overview of model. Simulations of sensory neurons and neural recordings are used to define the left and right inputs DeltaIl(t) and DeltaIr(t) to neural integrators. These inputs are modeled by Gaussian [Ornstein-Uhlenbeck (OU)] processes, which capture noise in the encoding of the motion strength by each pool of spiking neurons (seeNoneNone-Nonefor definition of input signals). Similar toMazurek et al. (2003), the activity levels of the left and right integratorsEl(t) andEr(t) encode accumulated evidence for each alternative. In the reaction time task,El(t) andEr(t) race to thresholds to determine choice on each trial. In the controlled duration task, the choice is made in favor of the integrator with higher activity at the end of the stimulus presentation. Depending on the task paradigm, different criteria may be used to terminate accumulation and give a decision. In the reaction time task, accumulation continues until activity crosses a decision threshold: if the leftward evidence integrator reaches threshold first, a decision that overall motion favored the leftward alternative is registered. In a second task paradigm, the controlled duration task, motion viewing duration is set in advance by the experimenter. A choice is made in favor of the integrator with greater activity at the end of the stimulus duration. Accuracy is defined as the fraction of trials that reach a correct decision. Speed is measured by the time taken to cross threshold starting from stimulus onset. Reaction time (RT) is then defined as the time until threshold (decision time) plus 350 ms of nondecision time, accounting for other delays that add to the time taken to select an alternative [e.g., visual latencies, or motor preparation time, cf. Mazurek et al. (2003)andLuce (1986)]. The exact value of this parameter was not critical to our results. Task difficulty is determined by the fraction of coherently moving dotsC(Britten et al. 1992;Mazurek et al. 2003;Roitman and Shadlen 2002). Accuracy and reaction time across multiple levels of task difficulty define the accuracy and chronometric functions in the reaction time task and together can be used to assess model performance. When necessary, these two numbers can be collapsed into a single metric, such as the reward per unit time or reward rate. In the controlled duration task, the only measure of task performance is the accuracy function. We now describe in detail the signals that are accumulated by the integrators corresponding to the left and right alternatives. First, we model the pools of leftward or rightward direction-selective sensory (MT) neurons as 100 weakly correlated [Pearson's correlation rho = 0.11 (Zohary et al. 1994;Bair et al. 2001)] spiking cells (seeFig. 3). As inMazurek and Shadlen (2002), neural spikes are modeled via unbiased random walks to a spiking threshold, which are correlated for neurons in the same pool. Increasing the variance of each step in the random walk increases the firing rate of each model neuron; it was therefore chosen at each coherence value to reproduce the linear relationship between coherenceCand mean firing rate mul,rof the left and right selective neurons observed in MT recordings:NoneNoneHere the parametersr0,bl, andbrapproximate recordings from MT (Britten et al. 1993);r0= 20, and if evidence favors the left alternative,bl= 0.4 andbr= -0.2; if the right alternative is favored, these values are exchanged. Construction of Gaussian (OU) processes to represent fluctuating, trial-by-trial firing rate of a pool of weakly correlated MT neurons (Bair et al. 2001;Zohary et al. 1994). As inMazurek and Shadlen (2002), these motion sensitive neurons provide direct input to our model integrator circuits. Simulated spike trains from weakly correlated, direction selective pools of neurons are shown as a rastergram. All spikes before timet, a sum over thejth spike from theith neuron, for alliandj, are convolved with an exponential filter, and then summed to create a continuous stochastic output (right); here,H(t) is the Heaviside function. We approximated this output by a simpler Gaussian (OU) process to simplify numerical and analytical computations that follow. Next, the output of each spiking pool was aggregated. Each spike emitted from a neuron in the pool was convolved with an exponential filter with time constant 20 ms, an approximate model of the smoothing effect of synaptic transmission. These smoothed responses were then summed to form a single stochastic process for each pool [seeFig. 3,right, andSmith (2010)]. We then approximated the smoothed output of each spiking pool by a simpler stochastic process that captures the mean, variance, and temporal correlation of this output as a function of dot coherence. We used Gaussian processesIl(t) andIr(t) for the rightward- and leftward-selective pools (seeFig. 3). Specifically, we chose Ornstein-Uhlenbeck (OU) processes, which are continuous Gaussian processes generated by the stochastic differential equationsNoneNonewith mean mul,r(C) as dictated byNoneand noise contributed by the Wiener processWNone. The variance mul,r(C) and timescale tau were chosen to match the steady-state variance and autocorrelation function of the smoothed spiking process. As shown inresults, this timescale affects the speed and accuracy of decisions under robust integration. Our construction so far accounts for variability in output from left vs. right direction selective neurons. We now incorporate an additional noise source into the output of each pool. These noise terms [etal(t) and etar(t), respectively] could represent, for example, neurons added to each pool that are nonselective to direction or intrinsic variability in the integrating circuit. Each noise source is modeled as an independent OU process with mean 0, timescale 20 ms as above, and a strength (variance) nugamma/2. This noise strength is a free parameter that we vary to match behavioral data (seeParameterizing and Comparing the Robust Integrator Model with Behavioral DataandFig. 15). We note that previous studies (Shadlen et al. 1996;Mazurek et al. 2003;Cohen and Newsome 2009) also found that performance based on the direction-sensitive cells alone can be more accurate than behavior and, therefore, incorporated variability in addition to the output of left and right direction selective MT cells. Finally, the signals that are accumulated by the left and right neural integrators are constructed by differencing the outputs of the two neural pools:NoneNoneA central issue is the impact of variability in the relative tuning of recurrent feedback vs. decay in a neural integrator circuit. Below, we will introduce the mistuning parameter beta, which determines the extent to which feedback and decay fail to perfectly balance. We first define the dynamics of the general integrator model on which our studies are based. This is described by the firing ratesEl,r(t) of integrators that receive outputs from left-selective or right-selective poolsIl,r(t) respectively. The firing ratesEl,r(t) increase as evidence for the corresponding task alternative is accumulated over time:NoneNoneThe three terms in this equation account for leak, feedback excitation, and the sensory input (scaled by a weight kappa = 1/9), with time constant tauNone= 20 ms. When the mistuning parameter beta = 0, leak and self-excitation exactly cancel, and hence the integrator is perfectly tuned. An integrator with beta ? 0 is said to be mistuned, with either exponential growth or decay of activity (in the absence of input). Imprecise feedback tuning is modeled by randomly setting beta to different values from trial to trial (but constant during a given trial), with a mean value beta? and a precision given by a standard deviation sigmabeta. We assume that beta? = 0 for most of the study. Thus the spread of beta, which we take to be Gaussian, represents the intrinsic variability in the balance between circuit-level feedback and decay. Perfect tuning corresponds to sigmabeta= beta? = 0, while sigmabeta? 0 or beta? ? 0 corresponds to a mistuned integrator. Finally, we set initial activity in the integrators to zero [El,r(0) = 0], and impose reflecting boundaries atEr= 0,El= 0 [as in, e.g.,Smith and Ratcliff (2004)] so that firing rates never become negative. The tradeoff between robustness to mistuning and sensitivity to inputs is described by the extended modelNoneAll subsequent results are based on this simplified model, which captures the essence of the robust integration computation. The first line is analogous to the series of potential wells depicted inFig. 1: if the sum of the mistuned integrator feedback and the input falls below the robustness limitR, the activity of the integrator remains fixed. If this summed input exceedsR, the activity evolves as for the nonrobust integrator inNone. To interpret the robustness limitR, it is convenient to normalize by the standard deviation of the input signal:NonewhereS=None. In this way,R? can be interpreted in units of standard deviations of input OU process that are "ignored" by the integrator. This yields the final expression, which we reproduce for convenience:NoneNoneTo summarize,Nonedefines a parameterized family of neural integrators, distinguished by the robustness limitR? AsR? - greater than 0, the model reduces toNone. When additionally beta = 0, the (perfectly tuned) integrator computes an exact integral of its input:Nonethen yieldsEl,r(t) ? ? None0DeltaIl,r(t')dt'. We analyze this robust integrator model below. Monte Carlo simulations ofNonewere performed using the Euler-Maruyama method (Higham 2001), withdt= 0.1 ms. For a fixed choice of input statistics and threshold theta, a minimum of 10,000 trials were simulated to estimate accuracy and reaction time values. In simulations where sigmabeta greater than 0, results were generated across a range of beta-values and then weighted according to a normal distribution. The range of values was chosen with no less than 19 linearly spaced points, across a range of +/-3 SD around the mean beta?. Simulations were performed on NSF Teragrid clusters and the UW Hyak cluster. Reward rate values presented inReward Rate and the Robustness-Sensitivity Tradeoffare presented as maximized by varying the free parameter theta; values were computed by simulating across a range of theta values. The range and spacing of these values were chosen dependent on the values ofR? and beta for the simulation; the range was adjusted to capture the relative maximum of reward rate as a function of theta, while the spacing was adjusted to find the optimal theta value with a resolution of +/-0.1: the values of theta and nugamma(see table included inFig. 15) were chosen to best match accuracy and chronometric functions to behavioral data reported inRoitman and Shadlen (2002). This was accomplished by minimizing the sum-squared error in data vs. model accuracy and chronometric curves across a discrete grid of theta and nugammavalues, with a resolution of 0.1. Autocovariance functions of integrator input presented inAnalysis: Robust Integrators and Decision Performancewere computed by simulating an Ornstein-Uhlenbeck process using the exact numerical technique inGillespie (1996)withdt= 0.1 ms to obtain a total of 227 sample values. Sample values of the process less than the specified robustness limit R? were set to 0, and the autocovariance function was computed using standard Fourier transform techniques. A general issue affecting neural systems that integrate stimuli is the balance between mechanisms that lead their activity to decay vs. grow over time (Fig. 1None). If these are tuned to a perfect balance, the result is graded persistent activity that can accumulate and store inputs. Robust integrators provide an alternative to such fine tuning but at the cost of lost sensitivity to input signals, represented by the energy wells inFig. 1None. Below we explore the costs and benefits of robust integration in terms of the general neural integrator model ofNone. This model summarizes the underlying issues via two key parameters. The first, beta, describes mistuning of the integrator away from "perfect" dynamics, so that its activity decays or grows autonomously (Fig. 1None). We describe the extent of mistuning by sigmabeta, which represents the standard deviation of beta from the ideal value beta? = 0. The second key parameter is the robustness limitR? . We emphasize dual effects ofR? : asR? increases, the integrator becomes able to produce a range of graded persistent activity for ever-increasing levels of mistuning (seeFig. 1None, whereR? corresponds to the depth of energy wells). This prevents runaway increase or decay of activity when integrators are mistuned; intuitively, this might lead to better performance on sensory accumulation tasks. At the same time, asR? increases, a larger proportion of the evidence fails to affect the integrator (seeFig. 1None, whereR? specifies a limit within which inputs are ignored). Such sensitivity loss should lead to worse performance. This implies a fundamental tradeoff between competing desiderata:1) one would prefer to integrate all relevant input, favoring smallR? , and2) one would prefer an integrator robust to mistuning (e.g., sigmabeta greater than 0), favoring large R?. Thus it makes sense to assess the effect of robustness under different degrees of mistuning, as represented schematically inFig. 4. Parameter space view of 4 integrator models, with different values of the robustness limitR? and feedback mistuning variability sigmabeta. The impact of transitioning from one model to another by changing parameters is either to enhance or diminish performance or to have a neutral effect (see text). To assess this performance, we consider relationships between decision speed and accuracy in both controlled duration and reaction time tasks. In the controlled duration task, we simply vary the stimulus presentation duration and plot accuracy vs. experimenter-controlled stimulus duration. In the reaction time task, we vary the decision threshold theta, treated as a free parameter, over a range of values, thus tracing out the parametric curve for all possible pairs of speed and accuracy values. Here, speed is measured by reaction time (seematerials and methods). For both cases, we use a single representative dot coherence (C= 12.8 inNone); similar results were obtained using other values for motion strength (seeFig. A4). We first establish the performance impact of mistuning in the absence of robustness. We begin with a case we call the "baseline" model (Fig. 4), for which there is no mistuning or robustness: sigmabeta=R? = 0: Speed accuracy plots for this model are shown as filled dots inFig. 5,AandB, for the controlled duration and reaction time tasks, respectively. We compare the baseline model with the "mistuned" model, indicated by crosses, for which the feedback parameter has a standard deviation of sigmabeta= 0.1 (i.e., 10% of the mean feedback) and robustnessR? = 0 remains unchanged. In the controlled duration task (Fig. 5None), we observe that mistuning diminishes accuracy by as much as 10%, and this effect is sustained even for arbitrarily long viewing windows (Usher and McClelland 2001;Bogacz et al. 2006). The same effect is apparent in the reaction time task (Fig. 5None): for a fixed reaction time, the corresponding accuracy is decreased. Mistuned feedback diminishes decision performance. Inset: plots depict a move in parameter space from the baseline model to the mistuned model by changing sigmabeta= 0 - greater than 0:1. In this and subsequent plots, simulation results are given with markers; lines are rational polynomial fits. A: in the controlled duration task, accuracy is lower for the mistuned model than for the baseline model at every trial duration, indicating a loss of performance when sigmabetaincreases. B: in the reaction time task, we parametrically plot all [reaction time (RT), accuracy] pairs attained by varying the decision threshold theta. Once again, accuracy is diminished by mistuning for a fixed mean reaction time. While maintaining feedback mistuning, we next increase the robustness limit toR? = 1.25. We call this case the "recovery" model because robustness compensates in part for the performance loss due to feedback mistuning: the speed accuracy plots inFig. 6for the recovery case, indicated by stars, lie above those for the mistuned model. For example, at the longer controlled task durations (Fig. 6None) and reaction times (Fig. 6None) plotted, 30% of the accuracy lost due to integrator mistuning is recovered via the robustness limitR? = 1.25. This degree of improvement underestimates the recovery attainable in some more realistic models. Indeed, a less simplified model of the integrator achieves a larger performance recovery (approaching ?75%; seeNoneandFig. A4). Increasing the robustness limitR? helps recover performance lost due to feedback mistuning. Results of simulation are plotted with fitting lines. Inset: we illustrate this by moving in parameter space from the mistuned model to the recovery model, by changingR? = 0 - greater than 1.25. The impact on decision performance is shown for both the controlled duration (A) and reaction time (B) tasks. We find thatR? greater than 0 yields a performance gain for the recovery model compared with the mistuned model (i.e., for a fixed accuracy, mean reaction time is increased). One possible reason for the modest recovery of accuracy inFig. 6is that robustness itself reduces decision speed and accuracy. However, this does not appear to be a viable explanation. When the same degree of robustness accompanies a perfectly tuned integrator, the "robust" case inFig. 4, there is negligible loss of performance. In particular,Fig. 7demonstrates that when sigmabeta= 0, speed accuracy curves forR? = 1.25 almost coincide with those for the baseline case ofR? = 0. We note that sinceR? measures ignored input in units of the standard deviation, the integrator circuit disregards as much as 75% of the input stimulus at low coherence values. Given this large amount of ignored stimulus, the fact that the robust integrator produces nearly the same accuracy and speed as the baseline case is surprising, as one might expect ignoring stimulus to be detrimental to performance. This implies that the robust model can protect against feedback mistuning, without substantially sacrificing performance when feedback is perfectly tuned. IncreasingR? alone does not compromise performance. Only simulation results, without fitting lines, are plotted for clarity. Inset: we illustrate this by moving in parameter space directly from the baseline to the "robust" model. For both controlled duration (A) and reaction time tasks (B), we plot the relationship between mean reaction time and accuracy. Circles give results for the baseline model, and "x" and "y" markers for the robust model atR? = 1 and 1.25, respectively. These curves are very similar in the baseline case, indicating little change in decision performance due to the robustness limitR? = 1.25. To summarize, the robust integrator appears well suited to the decision tasks at hand, countering some of the performance lost when feedback is mistuned. Moreover, even without mistuning, a robust integrator still performs as well as the baseline case that integrates all information in the input signal. In the next section, we begin to explain this observation by constructing several simplified models and employing results from statistical decision-making theory. We can begin to understand the effect of the robustness limit on decision performance by formulating a simplified version of the evidence accumulation process. We focus first on the controlled duration task, where the analysis is somewhat simpler. Readers preferring a brief statement of the underlying mechanisms may skip to theSummary of analysisand proceed to the rest of the study from there. Our first simplification is to consider a single accumulatorE, which receives evidence for or against a task alternative. Moreover, we consider increments of evidence that arrive both independently and discretely in time. The value ofEon theith time step,ENone, is allowed to be either positive or negative, corresponding to accumulated evidence favoring the leftward or rightward alternatives, respectively. On each time step,ENoneincrements by an independent, random valueZNonewith a probability density function (PDF)fNone(Z). We first describe an analog of the baseline model above (i.e., in the absence of robustness,R? = 0). Here, we take the incrementsZNoneto be independent, identical, and Gaussian distributed, with a mean mu greater than 0 (establishing the preferred alternative) and standard deviation sigma: that is,ZNone? N(mu, sigma2). After thenth step, we haveNoneIn the controlled duration task, a decision is rendered after a fixed number of time stepsN, (i.e.,n=N), and a correct decision occurs whenENone greater than 0. By construction,ENone? N(nmu,nsigma2), which implies that accuracy can be computed as a function of the signal-to-noise ratio (SNR)Noneof a sample:NoneNoneNext, we change the distribution of the accumulated incrementsZNoneto construct a discrete time analog of the robust integrator. Specifically, increasing the robustness parameter toR greater than 0 affects incrementsZNoneby redefining the PDFfNone(Z) so that weak samples do not add to the total accumulated "evidence," precisely as inNone. [Models where such a central "region of uncertainty" of the sampling distribution is ignored were previously studied in a race-to-bound model (Smith and Vickers 1989); seediscussion]. This requires reallocating probability mass below the robustness limit to a weighted delta function at zero (Fig. 8None). Specifically:NoneNoneR? affects the discrete time increment distribution. The probability density function of the random variableZNone, with probability mass for values between the robustness limitR? reallocated as a delta function centered at zero (in this figure,R? = 1). To estimate decision accuracy with robustnessR? greater than 0, we sumNrandom increments from this distribution forming the cumulative sumENone. As above, a correct decision occurs on trials whereENone greater than 0. As shown inFig. 9, the replacement of increments with zeros has negligible effect on accuracy whenR? is less than ?0.5. For larger values ofR? , accuracy diminishes faster for the discrete/independent model (light curve) compared with the continuous/correlated model (dark curve). Loss of accuracy is expected for both models as robustness effectively prevents stimulus information from affecting the decision. However, the continuous model suffers less than the discrete approximation, owing to the one important difference: the presence of temporal correlations in the evidence stream. We will return to this matter below. First, we give an explanation for the negligible effect ofR? on decision accuracy for either model. Accuracy of the discrete/independent and continuous/correlated models for the controlled duration task,T= 500 ms.A: approximation of the performance of the continuous/correlated model viaNoneis plotted as a black curve, and that predicted by the discrete/independent model with identical signal increments is plotted as a gray curve. B: disparity in performance of these 2 models can be partially understood by observing the decorrelating effect ofR? on the autocorrelation function for the evidence stream in the continuous/correlated model. Inset: 2 of these same functions (forR? = 0 andR? = 1.5) are plotted normalized to their peak value. The central limit theorem allows us to approximate the new cumulative sumENoneas a normal distribution (for sufficiently largeN), with mu and sigma inNonereplaced by the mean and standard deviation of the PDF defined byNone. As before, we normalizeRby the standard deviation of the increment,R? =R/sigma, and then express the fraction correct AccuracyNoneas a function ofR? ands. One can think ofR? as perturbing the original accuracy function given inNone. Although this perturbation has a complicated form, we can understand its behavior by observing that its Taylor expansion (seeDerivation ofNoneandNonefor more details) does not have first or second-order contributions inR? :NoneNoneThus, for small values ofR? (giving very smallR? 3), there will be little impact on accuracy. Nonecan therefore partially explain the key observation inFig. 7NonethatR? can be substantially increased while incurring very little performance loss. Now we return to the comparison between the discrete/independent and continuous/correlated models, by setting the SNR of the sampling distribution identical to the steady-state distribution of the input signal to the neural integrator model (seeSensory Input). The interval between samples is set to match accuracy performance of the continuous time model atR? = 0. The gray line inFig. 9Noneshows the accuracy for the discrete time model, as the robustness limitR? is increased. The discrete time model predicts a decrease in accuracy atR? = 0.5; intriguingly, this is not seen in an analogous continuous time model. In the next sections, we first describe this continuous time model and explain how the discrepancy in the impact of robustness can be resolved by accounting for the temporally correlated structure of the continuous time signal. We next extend the analysis of the controlled duration task in the previous section to treat integration of temporally correlated signals. In particular, we consider the continuous time signals described inSensory Inputabove (although our conclusions would apply to temporally correlated processes in discrete time as well). We follow the method developed inGillespie (1996)to describe the mean and variance of the integral of a continuous input signal. The challenge here lies in the temporal correlations in the Gaussian OU input signal (seeSensory Input). As in the previous section, we describe the distribution of the integrated signal at the final timeT. We first replace the discrete input samplesZNonefrom the previous section with a continuous signalZ(t), which we take to be a Gaussian process with a correlation timescale derived from our model sensory neurons (seematerials and methods). We define the integrated processNoneNonewith initial conditionE(0) = 0. Assuming thatZ(t) satisfies certain technical conditions that are easily verified for the OU process [wide-sense stationarity, alpha-stability, and continuity of sample paths (Gardiner 2002;Billingsley 1986;Gillespie 1996)], we can construct differential equations for the first and second moments less than E(t) and E2(t) greater than evolving in time. We start by taking averages on both sides of our definition ofE(t) and, noting thatE(0) = 0, compute the time-varying mean:NoneNoneSimilarly, we can derive a differential equation for the second moment ofE(t):NoneNoneThe right-hand side of this equation can be related to the area under the autocovariance functionA(tau) less than Z(t)Z(t+ tau) - Z(t) greater than 2of the processZ(t):NoneNoneWe now have an expression for how the second moment evolves in time. We can simplify the result via integration by parts and the fact that less than Z(t) greater than is constant in time:NoneNoneBecauseE(t) is an accumulation of Gaussian random samplesZ(t), it will also be normally distributed and hence fully described by the mean (None) and variance (None) (Billingsley 1986). To model a nonrobust integrator, we takeZ(t) to be a OU process with steady-state mean and variance mu and sigma2, and time constant tau. For the robust case, we can followNoneand parameterize a family of processesZNone(t) with momentary values below the robustness limitR? set to zero. (Here, we again normalize the robustness limit by the standard deviation of the OU process.) We numerically compute the autocovariance functionsANone(tau) of these processes and use the result to compute the required mean and variance, and hence time-dependent signal-to-noise ratio SNR(t), for the integrated processE(t). This yieldsNoneNoneUnder the assumption thatE(T) is approximately Gaussian for sufficiently longT(which can be verified numerically), we use this SNR to compute decision accuracy atT:NoneNoneThis function is plotted forT= 500 ms as the black line inFig. 9None. The plot shows that accuracy remains relatively constant until the robustness limitR? exceeds ?1.25, a longer range ofR? values than for the discrete time case (compare gray curve vs. black curve inFig. 9None). Why does the robustness limit appear to have a milder effect on degrading decision accuracy for our temporally correlated vs. independent input signals? We can get some insight into the answer by examining the autocovariance functionsAR? (tau), which we present inFig. 9None. When normalized by their peak value, the autocovariance forR? greater than 0.5 falls off more quickly with respect to the time lag tau (seeFig. 9None,inset), indicating that subsequent samples become less correlated in time. Thus there are effectively more "independent" samples that are drawn over a given time rangeT, improving the fidelity of the signal and hence decision accuracy. Such an improvement clearly has no room to occur when samples are already independent, as in the preceding section. Interestingly, this argument, that, all else being equal, the sum of samples with lower correlation will yield better decision accuracy, is the same as that applied to samples pooled across neural populations by (Zohary et al. 1994;Averbeck et al. 2006). The impact of the robustness limit on temporally correlated signals can be visualized by considering how robustness transforms a set of correlated (Gaussian) random variables, each representing the value of the signal at a nearby point in time (Fig. 10). As proven by (Lancaster 1957) and applied in a different context by (de la Rocha et al. 2007;Dorn and Ringach 2003), the nonlinear, thresholding action of this function must reduce the correlation of these variables;Fig. 10demonstrates this explicitly. This is the mechanism behind the decrease in autocorrelation above. Distribution of 2 neighboring (in time) samples of the incoming signal. A: before application of the robustness operation, the samplesZ(t1) andZ(t2) covary with correlation coefficient Corr = 0:5. B: after applying robustness (R? = 1), samples withZ(t1) less than R? are mapped toZNone(t1) = 0, and likewise for samples withZ(t2) less than R? . As a consequence, samples covary less in the region where either sample has been thresholded to zero, yielding Corr = 0:42. As pointed out to us by a reviewer, the same setup leads to a complementary view of why the robustness operation can have minimal impact on decision accuracy. The more correlated a set of random samples is, the greater the chance that they will have the same deviation from the mean, i.e., that nearby samples in time will all provide evidence in favor of the same task alternative. The limited effect of robustness on decision accuracy suggests that, even if some of these samples are rectified to zero, other (correlated) samples remain that lead to the same final decision. In summary, our analysis of decision performance for the controlled duration task shows that two factors contribute to the preservation of decision performance for robust integrators. The first is that the momentary SNR of the inputs is barely changed for robustness limits up toR? ? 0.5. The second is that, asR? increases, the signalZNone(t) being integrated becomes less correlated in time. This means that (roughly) more independent samples will arrive over a given time period. In the context of threshold crossing in the reaction time task, the accumulation of increments toward decision thresholds can be understood as the sequential probability ratio test, where the log odds for each alternative are summed until a predefined threshold is reached (Wald 1945;Gold and Shadlen 2002;Luce 1963;Laming 1968). Wald (1944)provides an elegant method of computing decision accuracy and speed (reaction time). The key quantity is given by the moment generating function [MGF, denotedMNone(omega) below] for the samplesZ[seeLuce (1986),Link and Heath (1975), andDoya (2007)]. Under the assumption that thresholds are crossed with minimal overshoot of the accumulator on the final step, we have the following expressions:NoneNoneNoneNonewhereh0is the nontrivial real root of the equationMNone(omega) = 1 and theta is the decision threshold. We first consider the case of a nonrobust integrator, for which the samplesZare again normally distributed. In this case, we must solve the following equation to omega =h0:NoneNoneIt follows that omega = 0 and omega =h0= - 2 mu / sigma2provide the two real solutions of this equation. (Wald's Lemma ensures that there are exactly two such real roots, for any sampling distribution meeting easily satisfied technical criteria.) When the robustness limitR? greater than 0, we can again compute the two real roots of the associated MGF. Here, we use the increment distributionfNoneNone(Z) given byNone, for which all probability mass withinRof 0 is reassigned to 0. Surprisingly, upon plugging this distribution into the expressionMNone(omega) = 1, we find that omega = 0,h0continue to provide the two real solutions to this equation regardless ofR, as depicted inFig. 11None. In the discrete modelR? increases RT but not accuracy. A: the second real rooth0ofMNone(s) remains unchanged asR? increases from 0 - greater than 2 (lines are uniformly distributed in this range). This implies that in the reaction time task, no changes in the accuracy will be observed (seeNone). B: however, the speed accuracy tradeoff will be affected, onceE[ZNone] begins to diminish (seeNone). This performance loss begins forR? greater than 0.5, in contrast to the performance of the continuous time model (seeFig. 7None). This observation implies that1) accuracies (None) are unchanged asRis increased, and2) reaction times (None) only change whenE[ZNone] changes. In other words, the integrator can ignore inputs below an arbitrary robustness limit at no cost to accuracy; the penalty will be incurred in terms of reaction time, and this will only be significant whenE[ZNone] changes appreciably. This result holds for any distribution for which:NoneNoneit is straightforward to verify that the Gaussian satisfies this property (seeDerivation ofNonefor more details). How much of an increase in R is necessary to decreaseE[ZNone], the key quantity that alone controls performance loss? After substitutingR? =R/sigma, we again find only one term up to fifth order inR? ,NoneNoneindicating that modest amounts of robustness lead to only small changes inE[ZNone]. [This is similar to the controlled duration case, where small values ofR? will have little effect on AccuracyNone(N), cf. None; seeDerivation ofNoneandNonefor more details.] This property, in combination with the constancy ofh0, allows us to reason about the tradeoff of speed vs. accuracy under robustness. Under symmetrically bounded drift-diffusion, accuracy is determined by theta andh0, whereas the mean decision time is determined by theta,h0, andE[Z]. Sinceh0is fixed for all values ofR? , the predicted effect of robustness is a slowing of the decision time owing to the small change inE[ZNone] (None). This effect is depicted inFig. 10None. The solid curve is a locus of speed-accuracy combinations achieved by varying theta under no robustness. As in the previous section, the SNR of the independent sampling distribution is identical to the continuous time model (and performance atR? = 0 is matched toFig. 7Noneby varying the time intersampling time, here 37 ms). Performance begins to decrease atR? = 0.5, and is much lower atR? = 1 than the continuous time model with correlated evidence streams. As in the preceding section, robustness serves to decorrelate this input stream, effectively giving more independent samples and preserving performance beyond that predicted by the independent sampling theory. In the preceding three sections we have analyzed the impact of the robustness limit on decision performance. For both the controlled duration and reaction time tasks, we first studied the effect of this limit on the evidence carried by momentary values of sensory inputs. In each task, this effect was more favorable than might have been expected. In the controlled duration case, the SNR of momentary inputs was preserved for a fairly broad range ofR, while in the reaction time task,Raffected speed but not accuracy at fixed decision threshold. These results provided a partial explanation for the impact of robustness on decisions. The rest of the effect was attributed to the fact that the robustness mechanism serves to decorrelate input signals in time, further preserving decision performance by providing the equivalent of more independent evidence samples in a given time window. Up to now, we have examined performance in the reaction time task by plotting the full range of attainable speed and accuracy values. The advantage of this approach is that it demonstrates decision performance in a general way. An alternative, more compact approach, is to assume a specific method of combining speed and accuracy into a single performance metric. This approach is useful in quantifying decision performance and rapidly comparing a wide range of models. Specifically, we use the reward rate (RR) (Gold and Shadlen 2002;Bogacz et al. 2006):NoneNonethe number of correct responses made per unit time, where a delayTdelimposed between responses to penalizes rapid guessing. Implicitly, this assumes a motivation on the part of the subject that may not be true; in general, human subjects seldom achieve optimality under this definition as they tend to favor accuracy over speed in two-alternative forced choice trials (Zacksenhouse et al. 2010). Here, we simply use this quantity to formulate a scalar performance metric that provides a clear, compact interpretation of reaction time data. Figure 12Noneshows accuracy vs. speed curves at four levels ofR? . The heavy solid line corresponds to the baseline model with robustness and mistuning set to zero (seeFig. 4). The lighter solid line corresponds to the mistuned model with sigmabeta= 0.1. The remaining broken lines correspond to the recovery model for three increasing levels of the robustness limitR? . Also plotted in the background as dashed lines are reward rate level curves, that is, lines along which reward rate takes a constant value, withTdel= 3 s. On each accuracy vs. speed curve, there exists a reward rate-maximizing (reaction time, accuracy) pair. This corresponds to a tangency with one reward rate level curve and is plotted as a filled circle. In general, each model achieves maximal reward rate via a different threshold theta; values are specified in the legend ofFig. 12None. [A general treatment of reward rate-maximizing thresholds for drift-diffusion models is given inBogacz et al. (2006).] Robustness improves reward rate (RR) under mistuning. A: speed accuracy curves plotted for multiple values ofR? ; as in previous figures, the greater accuracies found at fixed mean reaction times indicate that performance improves asR? increases. The heavy line indicates the baseline case of a perfectly tuned, nonrobust integrator (repeated fromFig. 5None). RR level curves are plotted in background (dotted lines; see text), and points along speed accuracy curves that maximize RR are shown as circles. These maximal values of RR are plotted inB, demonstrating the nonmonotonic relationship betweenR? and the best achievable RR. In sum, we see that mistuned integrators with a range of increasing robustness limitsR? achieve greater reward rate, as long as their thresholds are adjusted in concert. The optimal values of reward rate for a range of robustness limitsR? are plotted inFig. 12None. Figure 12illustrates the fundamental tradeoff between robustness and sensitivity. If there is variability in feedback mistuning (sigmabeta greater than 0), increasingR? can help recover performance but only to a point. Beyond a certain level, increasingR? further starts to diminish performance, as too much of the input signal is ignored. Overall, the impact on levels of reward rate is small: often ?1%. We next consider the possibility that variation in mistuning from trial to trial could occur with a systematic bias in favor of either leak or excitation and ask whether the robustness limit has qualitatively similar effects on decision performance as for the unbiased case studied above. Specifically, we draw the mistuning parameter beta from a Gaussian distribution with standard deviation sigmabeta= 0.1 as above but with various mean values beta? (seematerials and methods). InFig. 13Nonewe show reward rates as a function of the bias beta?, for several different levels of the robustness limitR? . At each value of beta?, the highest reward rate is achieved for a value ofR? greater than 0; that is, regardless of the mistuning bias, there exists anR? greater than 0 that will improve performance vs. the nonrobust case (R? = 0). We note that this improvement appears minimal for substantially negative mistuning biases (i.e., severe leaky integration) but is significant for the values of beta? that yield the highest reward rate. Finally, the ordering of the curves inFig. 13Noneshows that, for many values of beta?, this optimal robustness limit is an intermediate value less thanR? = 2. Robustness improves performance across a range of mistuning biases beta?. In both the reaction time (A) and controlled duration (B) tasks, robustness helps improve performance when beta ? N(beta?, 0.12), for all values of beta? shown. As in previous figures, the coherence of the sensory input isC= 12.8. In the reaction time task (A), theta is varied for each value of beta? to find the maximal possible RR, and performance gains are largest for beta? greater than 0. In the controlled duration task, substantial gains are possible across the range of beta? values. WhileFig. 13only assesses performance via a particular performance rule (reward rate,Tdel= 3 s), the analysis inReward Rate and the Robustness-Sensitivity Tradeoffsuggests that the result will hold for other performance metrics as well. Moreover,Fig. 13Nonedemonstrates the analogous effect for the controlled duration task: for each mistuning bias beta?, decision accuracy increases over the range of robustness limits shown. We have demonstrated that increasing the robustness limitR? can improve performance for mistuned integrators, in both the reaction time and controlled duration tasks. In the latter, a decision was made by examining which integrator had accumulated more evidence at the end of the time interval. In contrast,Kiani et al. (2008)argue that decisions in the controlled duration task may actually be made with a decision threshold, much like the reaction time task. That is, evidence accumulates until an absorbing bound is reached, causing the subject to ignore any further evidence and simply wait until the end of the trial to report the decision. Figure 14demonstrates that our observations about how the robustness limit can recover performance lost to mistuned feedback carry over to this model of decision making as well. Specifically,Fig. 14Noneshows how settingR? greater than 0 improves performance in a mistuned integrator. In fact, more of the lost performance is recovered than in the previous model of the controlled duration task (cf. Fig. 6None). Figure 14Noneextends this result to show that some value ofR? greater than 0 will recover lost performance over a wide range of mistuning biases beta? (cf. Fig. 13None). Effect of the robustness limitR? on decision performance in a controlled duration task, under the bounded integration model ofKiani et al. (2008). Dot coherenceC= 12.8. A: increasing the robustness limitR? helps recover performance lost to mistuning at multiple reaction times in the controlled duration task. Specifically, moving from the baseline model to the mistuned model decreases decision accuracy, but this lost accuracy can be partially or fully recovered forR? greater than 0. B: when allowing for biased mistuning (beta? ? 0, sigma = 0.1),R? still allows for recovery of performance; effects are most pronounced when beta? greater than 0. We have demonstrated that robustness serves to protect an integrator against the hazards of runaway excitation and leak and that the cost of doing so is surprisingly small. Yet, it is hard to know whether the range of effects is compatible with known physiology and behavior. Without better knowledge of the actual neural mechanisms that support integration and decision making, it is not possible to directly reconcile the parameters of our analysis with physiology. However, in this study we required that the free parameter values in the models that we analyzed were compatible with measured psychophysics. To accomplish this, we fit accuracy and chronometric functions from robust integrator models to reaction time psychophysics data reported inRoitman and Shadlen (2002). This fit is via least squares across the range of coherence values and requires two free parameters for each integrator model: additive noise variance nugammaand the decision bound theta (seematerials and methods). Such noise and bound parameters are standard in fitting accumulator-type models to behavioral data. Figure 15shows the results. Figure 15,AandB, displays accuracy and chronometric data together with fits for various integrator models, withR? = 0. The solid line shows a close fit for the baseline model (i.e., with no feedback mistuning or robustness, seeFig. 4) to the behavioral data, in agreement with prior studies (Mazurek et al. 2003). The broken lines give analogous fits for mistuned models (sigmabeta= 0.1), with three values of bias in feedback mistuning (beta?). Accuracy (AandC) and chronometric (BandD) functions: data and model predictions. Solid dots and stars are behavioral data for a rhesus monkey [subject"N" and "B," respectively (Roitman and Shadlen 2002)]. InA-D, the accuracy and chronometric functions are fit to behavioral data via least squares, over the free parameters theta and nugamma. InAandB, the robustness thresholdR? = 0, and results are shown for baseline and exemplar mistuned models (see legend in table). InCandD, results are shown for the robust and recovery models (Ris fixed across the range of coherence values so thatR? = 1.25 atC= 0). The close matches to data points indicate that these models can be reconciled with the psychophysical performance of individual subjects by varying few parameters. Parameter values for each curve are summarized in the table. Figure 15,CandD, shows the corresponding results for robust integrators. For all cases in these panels, we take the robustness limitR? = 1.25. We fix levels of additive noise to values found for the nonrobust case above to demonstrate that by adjusting the decision threshold, one can obtain approximate fits to the same data. This is expected from our results above:Fig. 6shows that, while accuracies at given reaction times are higher for mistuned robust vs. nonrobust models, the effect is modest on the scale of the full range of values traced over an accuracy curve. Moreover, for the perfectly tuned case, accuracies at given reaction times are very similar for robust and nonrobust integrators (Fig. 7, with a slightly lower value ofR? ). Thus comparable pairs of accuracy and reaction time values are achieved for robust and nonrobust models, leading to similar matches with data. In sum, the accuracy and chronometric functions inFig. 15show that all of the models schematized inFig. 4, baseline, mistuned, robust, and recovery, are generally compatible with the chronometric and accuracy functions reported inRoitman and Shadlen (2002). A limitation of our analysis concerns the distribution of reaction time. Above we considered the effects of robustness on mean decision time but not on the shape of the distributions. The standard DDM predicts a longer tail to the reaction time distribution than is seen in data, thereby necessitating modifications to the simple model (Ditterich 2006;Ratcliff and Rounder 1998;Churchland et al. 2008). The most compelling modification in our view is a time-dependent reduction in the decision threshold theta. Our experience suggests that such modifications can also be implemented under robustness with and without mistuning. However, we did not attempt to fit the reaction time distributions from the experiments and leave the matter for future investigation. A wide range of cognitive functions require the brain to process information over time scales that are at least an order of magnitude greater than values supported by membrane time constants, synaptic integration, and the like. Integration of evidence in time, as occurs in simple perceptual decisions, is one such well-studied example, whereby evidence bearing on one or another alternative is gradually accumulated over time. This is formally modeled as a bounded random walk or drift diffusion process in which the state (or decision) variable is the accumulated evidence for one choice and against the alternative(s). Such formal models explain both the speed and accuracy of a variety of decision-making tasks studied in both humans and nonhuman primates (Ratcliff 1978;Luce 1986;Gold and Shadlen 2007;Palmer et al. 2005), and neural correlates have been identified in the firing rates of neurons in the parietal and prefrontal association cortex (Mazurek et al. 2003;Gold and Shadlen 2007;Churchland et al. 2008;Shadlen and Newsome 1996,2001;Schall 2001;Kim et al. 2008). The obvious implication is that neurons must somehow integrate evidence supplied by the visual cortex, but there is mystery as to how. This is a challenging problem because the biological building blocks operate on relatively short time scales. From a broad perspective, the challenge is to assemble neural circuits that that can sustain a stable level of activity (i.e., firing rate) and yet retain the capability to increase or decrease firing rate when perturbed with new input (e.g., momentary evidence). A well-known solution is to suppose that recurrent excitation might balance perfectly the decay modes of membranes and synapses (Cannon and Robinson 1983;Usher and McClelland 2001). However, this balance must be fine tuned (Seung 1996;Seung et al. 2000), or else the signal will either dissipate or grow exponentially (Fig. 1None,top). Several investigators have proposed biologically plausible mechanisms that mitigate somewhat the need for such fine tuning (Lisman et al. 1998;Goldman et al. 2003;Goldman 2009;Romo et al. 2003;Miller and Wang 2005;Koulakov et al. 2002). These are important theoretical advances because they link basic neural mechanism to an important element of cognition and thus provide grist for experiment. Although they differ in important details, many of the proposed mechanisms can be depicted as if operating on a scalloped energy landscape with relatively stable (low energy) values, which are robust to noise and mistuning in that they require some activation energy to move the system to a larger or smaller value [Fig. 1None,bottom; cf. Pouget and Latham (2002);Goldman et al. (2009)]. The energy landscape is a convenient way to view such mechanisms, which we refer to as robust integrators, because it also draws attention to a potential cost. The very same effect that renders a location on the landscape stable also implies that the mechanism must ignore information in the incoming signal (i.e., evidence). Here, we have attempted to quantify the costs inherent in this loss. How much loss is tolerable before the circuit misses substantial information in the input? How much loss is consistent with known behavior and physiology? We focused our analyses on a particular well-studied task because it offers critical benchmarks to assess both the potential costs of robustness to behavior and a gauge of the degree of robustness that might be required to mimic neurophysiological recordings with neural network models. Moreover, the key statistical properties of the signal and noise (to be accumulated) can be estimated from neural recordings. We first imposed a modest amount of mistuning in recurrent excitation and asked whether robustness could protect against loss of decision-making performance. We found that it could. Although in general this protection is only partial (Figs. 6, 19), for the controlled duration task it can be nearly complete depending on the presence of a decision bound (Fig. 14None, controlled duration greater than 3 s). In absolute terms, effect sizes are small for the task and mistuning levels that we modeled. A few percentage points of accuracy are lost due to mistuning and recovered due to robustness; this leads to only a small benefit in performance measures such as reward rate. However, our most surprising result is not the subtle improvement that robustness can confer in our simulated task but the fact that levels of robustness can be quite high before they begin to degrade decision-making performance. Both in the presence and in the absence of mistuning, ignoring a large part of the motion evidence either slightly improved performance or produced an almost negligible decrement. This was the case even when more than a full standard deviation of the input distribution is ignored; in fact, this is the level of robustness that produced the best performance in the presence of mild mistuning. We can appreciate the impact of robust integration by considering the distribution of random values that would increment the stochastic process of integrated evidence. Instead of imagining a scalloped energy surface, we simply replace all the small perturbations in integrated evidence with zeros. Put simply, if a standard integrator would undergo a small step in the positive or negative direction, a robust integrator instead stays exactly where it was. In the setting of drift diffusion, this is like removing a portion of the distribution of momentary evidence (the part that lies symmetrically about zero) and replacing the mass with a delta function at 0. At first glance this appears to be a dramatic effect, see the illustration of the distributions inFig. 8, and it is surprising that it would not result in strong changes in accuracy or reaction time or both. Three factors appear to mitigate this loss of momentary evidence. First, setting weak values of the input signal to zero can reduce both its mean and standard deviation by a similar amount, resulting in a small net change to the input SNR. Second, surprisingly, the small loss of signal-to-noise that does occur would not result in any loss of accuracy if the accumulation were to the same bound as for a standard integrator. The cost would be to decision time alone. Third, even this slowing is mitigated by the temporal dynamics of the input. Unlike for idealized drift diffusion processes, real input streams possess definite temporal correlation. Interestingly, removing the weakest momentary inputs reduces the temporal correlation of the noise component of the input stream. This can be thought of as allowing more independent samples in a given time period, thereby improving accuracy at a given response time. While we used a simplified characterization of the robust integration operation in our study, we noted that there are many different ways in which this could be realized biologically (Koulakov et al. 2002;Nikitchenko and Koulakov 2008;Goldman et al. 2003,2009;Fransen et al. 2006;Loewenstein and Sompolinsky 2003;Egorov et al. 2002). Inappendix a, we make this connection concrete, for one such mechanism based on bistable neural pools. An intriguing finding presented there (see Fig. ) is that the robustness mechanism provided by the circuit-based bistable model produces an even more favorable effect of robustness for decision making than the simplified model in the main text. This demonstrates the generality of our results and points to an intriguing area of future study, focusing on the impact of more detailed circuit- and cell-level dynamics. Our robust integrator framework shares features with existing models in sensory discrimination. The interval of uncertainty model ofSmith and Vickers (1989)and the gating model ofPurcell et al. (2010)ignore part of the incoming evidence stream, yet they can explain both behavioral and neural data. We suspect that the analyses developed here might also reveal favorable properties of these models. Notably, some early theories of signal detection also featured a threshold, below which weaker inputs fail to be registered, the so-called high threshold theory (reviewed inSwets 1961). The primary difference in the current work is to consider single decisions made based on an accumulation of many such thresholded samples (or a continuous stream of them). Although they are presented at a general level, our analyses make testable predictions. For example, they predict that pulses of motion evidence added to random dot stimulus would affect decisions in a nonlinear fashion consistent with a soft threshold. Such pulses are known to affect decisions in a manner consistent with bounded drift diffusion (Huk and Shadlen 2005) and its implementation in a recurrent network (Wong et al. 2008). A robust integration mechanism further predicts that brief, stronger pulses will have greater impact on decision accuracy than longer, weaker pulses containing the same total evidence. Beyond pulses with different characteristics, it is possible that an analogous thresholding effect could be seen for periods of strong and weak motion evidence in the random dots stimuli themselves, although this would require further study to assess. However, we believe that the most exciting application of our findings will be to cases in which the strength of evidence changes over time, as expected in almost any natural setting. One simple example is for task stimuli that have an unpredictable onset time and whose onset is not immediately obvious. For example, in the moving dots task, this would correspond to subtle increases in coherence from a baseline of zero coherence. Our preliminary calculations agree with intuition that robust integrator mechanism will improve performance: in the period before the onset of coherence, less baseline noise would be accumulated; after the onset of coherence, the present results suggest that inputs will be processed with minimal loss to decision performance. This intuition can be generalized to apply to a variety of settings with nonstationary sensory streams. Many cognitive functions evolve over time scales that are much longer than the perceptual decisions we consider in this study. Although we have focused on neural integration, it seems likely that many other neural mechanisms are also prone to drift and instability. Hence, the need for robustness may be more general. Yet, it is difficult to see how any mechanism can achieve robustness without ignoring information. If so, our finding may provide some optimism. Although we would not propose that ignorance is bliss, it may be less costly than one would expect. This research was supported by a Career Award at the Scientific Interface from the Burroughs-Wellcome Fund (to E. Shea-Brown), Howard Hughes Medical Institute, the National Eye Institute Grant EY-11378, and National Center for Research Resources Grant RR-00166 (to M. Shadlen), a seed grant from the Northwest Center for Neural Engineering (to E. Shea-Brown and M. Shadlen), NSF Teragrid Allocation TG-IBN090004, and in part by the University of Washington eScience Institute. No conflicts of interest, financial or otherwise, are declared by the author(s). Author contributions: E.S.-B., N.C., A.K.B., and M.S. conception and design of research; E.S.-B., N.C., A.K.B., and M.S. performed experiments; E.S.-B., N.C., A.K.B., and M.S. analyzed data; E.S.-B., N.C., A.K.B., and M.S. interpreted results of experiments; E.S.-B., N.C., A.K.B., and M.S. prepared figures; E.S.-B., N.C., A.K.B., and M.S. drafted manuscript; E.S.-B., N.C., A.K.B., and M.S. edited and revised manuscript; E.S.-B., A.K.B., and M.S. approved final version of manuscript.